# Data Partitioning 

_As your site's traffic grows, your database might end up being **Bottlenecked** as it gets Overloaded with requests._

**Vertically** Scaling your DB by upgrading the single database to something very powerful starts to get very **expensive**.

_This is where **Sharding** may help._

## Sharding

**Sharding**: Breaking you big single logical DB into **multiple small DBs**. 

**Load** is now **distributed** over multiple machines and the size of the DB is **more scalable** as you can just **add another node (instance)** to increase the size. 

#### Sharding can be implemented at both Application Level or The Database Level. 

***Cassandra, Hadoop, MongoDB** are examples of **Distributed Databases** which are **Natively Sharded**. 

**Redis** is an example of greate **In-Memory** Database whrere **Sharding** can be implemented at the **Application-Level**.


## Partitioning 

- There are two ways of Partitioning a DB: **Horzontally** and **Vertically** 

### 1. Vertical Partitioning 

- Vertical Partitioning is where we **split data by columns or features** 

- Example: For a Twitter, where we have Users, their tweets, followers, and Trending Topics:

- We can store **Users in one DB**, their **Tweets in another** and **Trending Topics in another DB**.

#### This is quite simple but is say User base grows, we might have to split the Users DB further. 

#### Thus Users DB need to be Split by Rows (Horizontal Partitioning).


### 2. Horizontal Parititioning 

- Horizontal Partitioning is called **Sharding**. 

- We **Split** data **by Rows**.

- Example: We Split all Users with name **A to M** in One DB. And users with name **N to Z** in Another DB. 

- A **Problem** here is that we can have **Unbalanced Service**.

- If lets say **75%** of user names are in range **A to M**, then our **A to M DB** will take 75% of the load. 

![db-part1](./db_part_h_v_v.png)


## Data Partitioning Criteria 

### How do you Route read/write operations to-and-from your Distributed Database Architecture (Partitioned DB)

Two common approaches are: **Algorithmic** and **Dynamic** Sharding. 

### 1. Algorithmic Sharding: 

- The Servers uses a **Sharding function** (A **Hashing** function basically) on requests ID, and uses **modulo (%)** with the **number of db instances**. 

- This poses some problems when a **New Instance of DB** is introduced. (See Load Balancer)

- A better algo such as **Consistent Hashing** is used. 

![db-part2](./db_part_algo_shard.png)


### 2. Dynamic Sharding: 

- In Dynamic Sharding we have a **Lookup Service** which holds the **locations** of all our entities.

- All **requests go** to the **lookup service**, where the **partition keys** are used to identify which DB to **route the request to**.

- This has a **Single Point of Failure**. If the **Lookup Service** goes down, no DB can be accessed. 

![db-part3](./db_part_dynamic_shard.png)



## Data Partitioning/Sharding Problems

1. **Shard Allocation Imbalance**:
    Data is not spread evenly across all shards 

2. **Hot Key**: 
    Once particular key is **accessed excessively**. 
    
    Imagine some celebrity's tweet has gone viral. 
    Since that user's tweets are all stored in one partition, A large amount of users will keep request that tweet, which will overload that one particular node (partition).

3. **Data Redistribution**: 
    Redistributing data whenever adding/removing servers or changing sharding criteria would **require downtime** in system. 

4. **Require Data Denormalization**:
    In a Single DB, queries that require **Joins** can be done **easily**. 

    But if you **Shard** your DB, you **sacrifice this ease**.

    **To avoid** needing to use **Joins** you'll have to **Denormalize your data sets** before **sharding**.  



### Sharding introduces some complexities and problems. 

### Before sharding, you should consider some other options such as "Indexing" or "Caching". 