# Caching 

### A Cache is a high-speed data storage layer which stores a subset of data, so that future requests of that data are served up faster than is possible by accessing the data's primary storage location (like DB).

**Caching allows you to efficiently reuse previously retrieved or computed data.**


### How Caching Works ? 

The data in a cache is generally stored in **fast access harware** such as **RAM**. 

**A cache's primary purpose is to increase data retrieval performance by reducing the need to access the underlying slower storage layer.** 

**Cache's are usually key-value stores, that are stored in-memory**

### Four Scenarios When To Use Cache: 

1. When you're querying for some commonly used data. 
    - This saves Network calls. 
    - eg. Top 10 movies of week For Imdb.

2. When you want to avoid computations. 
    - This saves expensive recomputations. 
    - eg. Average age of all users can be stored. 

3. When you want to avoid load on the database. 
    - This saves DB load. 
    - eg. Store homepage data in cache (Trending topics etc.)

4. When you want to speed up response times to client (user)
    - Saves user's time. Make service appear faster. 


#### Do not store everything on cache

- Cache Hardware is expensive (SSD's)

- You also need so search data in Cache. With increase in Cache data size, search time increases (point of cache becomes redundant)

#### The Database has Infinite information. The Cache has the most Relevant information (according to the requests that will come in the future). 

**Thus Caching requires some prediction.**

To predict accurately, we need to know two things: 

- When do we load data into the cache ?
- When do we evict (remove) data from the cache ?

### A Cache Policy is the way we load/evict data from Cache

#### LRU Policy (Least-Recently-Used):

- Whenever we load/pull entries into the Cache we put it on the TOP. 

- Whenever we evict/pop entries from Cache we remove them from the BOTTOM. 

- Whenever an entry is used from Cache we move it to the TOP again. 

- This way as Cache entries go unused they start moving to BOTTOM. 

**This uses a custom Queue**


### LFU Policy (Least-Frequently-Used):

- Not used that much (google it.)


### Bad Cache Designs: 

#### What if you have very Poor Eviction Policy ?

- **Extra Calls** need to be made when hitting the cache is of no use. Whenever you ask cache for some data, it responds with "I don't have it". 
    
- Its actually harmful to have such a cache (you make extra call to cache for no reason)


#### What if you have a very Small Cache ?

- **Trashing** is a concept where you are loading and evicting data from cache without it being ever used from cache. 

- When UserA requests for his profile, which is fetched from DB first, it gets stored in Profile Cache. 

- When UserB requests for his profile, it is not found in Cache thus fetched from DB, it replaces UserA profile in cache due to small size. 

- When UserA requests again, his profile is not found in Cache. 


#### What about Consistency ? 

- When UserA updates his profile on the DB. His profile (already) stored in the Cache is outdated now. 

- If UserA's profile is fetched again from Cache, it will fetch outdated profile. 

- This leads to **Data Inconsitency**


## Where To Place The Cache ?