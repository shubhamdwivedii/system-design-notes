# Caching 

### A Cache is a high-speed data storage layer which stores a subset of data, so that future requests of that data are served up faster than is possible by accessing the data's primary storage location (like DB).

**Caching allows you to efficiently reuse previously retrieved or computed data.**


### How Caching Works ? 

The data in a cache is generally stored in **fast access harware** such as **RAM**. 

**A cache's primary purpose is to increase data retrieval performance by reducing the need to access the underlying slower storage layer.** 

**Cache's are usually key-value stores, that are stored in-memory**

### Four Scenarios When To Use Cache: 

1. When you're querying for some commonly used data. 
    - This saves Network calls. 
    - eg. Top 10 movies of week For Imdb.

2. When you want to avoid computations. 
    - This saves expensive recomputations. 
    - eg. Average age of all users can be stored. 

3. When you want to avoid load on the database. 
    - This saves DB load. 
    - eg. Store homepage data in cache (Trending topics etc.)

4. When you want to speed up response times to client (user)
    - Saves user's time. Make service appear faster. 


#### Do not store everything on cache

- Cache Hardware is expensive (SSD's)

- You also need so search data in Cache. With increase in Cache data size, search time increases (point of cache becomes redundant)

#### The Database has Infinite information. The Cache has the most Relevant information (according to the requests that will come in the future). 

**Thus Caching requires some prediction.**

To predict accurately, we need to know two things: 

- When do we load data into the cache ?
- When do we evict (remove) data from the cache ?

### A Cache Policy is the way we load/evict data from Cache

#### LRU Policy (Least-Recently-Used):

- Whenever we load/pull entries into the Cache we put it on the TOP. 

- Whenever we evict/pop entries from Cache we remove them from the BOTTOM. 

- Whenever an entry is used from Cache we move it to the TOP again. 

- This way as Cache entries go unused they start moving to BOTTOM. 

**This uses a custom Queue**


#### LFU Policy (Least-Frequently-Used):

- Not used that much (google it.)


### Bad Cache Designs: 

#### What if you have very Poor Eviction Policy ?

- **Extra Calls** need to be made when hitting the cache is of no use. Whenever you ask cache for some data, it responds with "I don't have it". 
    
- Its actually harmful to have such a cache (you make extra call to cache for no reason)


#### What if you have a very Small Cache ?

- **Trashing** is a concept where you are loading and evicting data from cache without it being ever used from cache. 

- When UserA requests for his profile, which is fetched from DB first, it gets stored in Profile Cache. 

- When UserB requests for his profile, it is not found in Cache thus fetched from DB, it replaces UserA profile in cache due to small size. 

- When UserA requests again, his profile is not found in Cache. 


#### What about Consistency ? 

- When UserA updates his profile on the DB. His profile (already) stored in the Cache is outdated now. 

- If UserA's profile is fetched again from Cache, it will fetch outdated profile. 

- This leads to **Data Inconsitency**


## Where To Place The Cache ?

Cache can be placed in two ways: 

- **Close to the Servers (In Memory)** 
- **Away from the Servers (Global Cache)** 

### 1. Close to the Servers (In Memory) 

We can put the cache _in-memory_ in a Server.

Some amount of memory in your servers is going to be **used-up** by the Cache.

But if the number of results that you are working is **really small** and you need to save on the network calls. Then you can just keep a Cache **in-memory**.

#### Advantages:
- Its very **faster**. Due to in-memory access. 
- It is **simpler to implement**.

#### Disadvantages: 
- Since cache is in-memory, if you are running multiple instances of you server (horizontally scaled). Each server will have **its own** version of cache, **distince caches**.

- If one of the instances (Server1) fails, the **in-memory cache** associated with it is **also lost**.

- You also need to deal with **inconsitencies** in cache data between two instances of server. 


### 2. Away from the Servers (Global Cache)

We keep a **separate** (away from servers) **global cache**. 

**Redis** is commonly used to implement Global Caches. 
Redis provides persistent storage (like a cache). It is a key-value store. 

All Servers hit the global cache. If data is not found, Global cache would query the database. Otherwise it would return data. 

_NOTE: In distributed systems we have **distributed global caches**_

#### Advantages: 
- If one of the server instances (ServerA) **goes down**, the Global Cache is **unaffected**. 

- Global Cache also maintains **data consitency**. 

- Although **slower**, Global Caches are **more accurate**.

- Global Caches can be **Scaled Independently**. You can add more instances of Global Caches. 

#### Disadvantages: 
- **Slower** than in-memory caches. 
- Not that easy to implement. 



## Maintaining Consitency 

There are two approaches: **Write Though Cache** & **Write Back To Cache**.

### Write-Through Caches: 

- When you need to update some data (say ProfileA), you hit the **cache first** if the data (ProfileA) **is in** the cache, you know that updating the data (ProfileA) in DB directly would lead to **inconsistency** 

- In such case you **update** the data (ProfileA) **in cache first** then you **push** the update to the DB. 

- Thus you **"write through"** the cache. 

#### Disadvantage: 

- This doesn't work well when the **cache is in-memory** and multiple instances of servers are running.
    - You can only update one instance's in-memory cache. 
    - You have to user **write-back** for **in-memory** caches.


### Write-Back To Caches: 

- When you need to update some data (say ProfileA), you hit the **DB directly** and make update there. After updating in DB directly, you **make an entry** to the **cache**. 

- You can either **update** the entry in the cache OR you can simple **evict/remove** the entry from cache. 

- Evicting/Removing the entry **is more common**. When user makes a subsequent request to get ProfileA, updated entry would be added to the cache after its fetched from the DB.

- Thus you **write back** to the cache after hitting **DB first**. 

#### Disadvantage: 

- Every time you update the DB, you have to **keep updating the cache** as well. This leads to low performance.
    - This can get expensive for write heavy systems. 


### Hybrid of Write-Through and Write-Back:

- Usually a hybrid of both is used, due to their disadvantages. 

- Requests hit the cache first, but they do not immediately go through to the DB. 

- **Updates are accumalated** in the Cache and then **after set intervals** are **written to the DB in bulk**.